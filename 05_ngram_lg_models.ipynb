{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a language model from ngrams\n",
    "\n",
    "We know that a language model is just the encoding of knowledge about likely sequences and word associations. You can create a language model by obtaining statistics about a large corpus of ngrams. You can also create a large language model (LLM) like the ones behind ChatGPT or Llama using deep learning. \n",
    "\n",
    "Here, we'll create simple language models from ngrams using NLTK. The intuition about n-grams is that you can predict the next _n_ in a sequence if you know the frequencies of pairs of _n_ items from corpora. To make things simple, let's think of _n_ as 2. And we will assume _n_ is a word. But you can also calculate ngram frequency for characters, sounds, or sentences. \n",
    "\n",
    "So, let's say that we are calculating 2-gram sequences of words. Then, we are calculating bigrams. There are also unigrams (one word at a time), trigrams (sequences of three words), 4-grams, etc. You get the idea. \n",
    "\n",
    "Thus, we can figure out what the next word is if we know the previous words are. Let's say that we want to find out the likelihood that the next word in the sequence _I really like_ is _you_. This is, by the way, what Google suggested when I typed _I really like..._ The first link was to a [Carly Rae Jepsen song](https://youtu.be/qV5lzRHrGeg). We can calculate that as:\n",
    "\n",
    "\n",
    "$$ P(you | I, really, like ) $$\n",
    "\n",
    "The way that formula is written is a 4-gram (a sequence of 4 words). This can be difficult to calculate, especially for less frequent combinations of sentences. So, to make this into a bigram probability, we calculate the following, which reads as \"the probability of _you_ given _like_\": \n",
    "\n",
    "$$ P(you | like ) $$\n",
    "\n",
    "The general formula is below. The probability of $w_i$ given the sequence $w_1$ to $w{i-1}$ is approximately the probability of  $w_i$ given $w_{i-1}$. So, instead of calculating probabilities for a long sequence of words, we do it for a sequence of 2 words at a time.\n",
    "\n",
    "$$ P(w_i | w_1, w_2, w_3, ..., w_{i-1} ) \\approx P(w_i | w_{i-1}) $$\n",
    "\n",
    "Note that above we say \"the probability of _x_ given _y_\". To calculate that, we just count how often any 2 words appear in a large enough corpus. This is what we'll do in this notebook!\n",
    "\n",
    "Credits: [NLTK LM documentation](https://github.com/nltk/nltk/blob/develop/nltk/lm/__init__.py), [N-gram language models](https://www.kaggle.com/code/alvations/n-gram-language-model-with-nltk), [N-gram language modelling with NLTK](https://www.geeksforgeeks.org/n-gram-language-modelling-with-nltk/), [Predicting next word using n-gram model NLTK](https://stackoverflow.com/questions/75565130/predicting-next-word-using-n-gram-model-nltk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements\n",
    "\n",
    "We import everything we need, including bits of NLTK. To train only on \"important\" or content words, we will remove punctuation and stopwords. We'll first use the [NLTK Reuters corpus](https://www.nltk.org/book/ch02.html#reuters-corpus) to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.corpus import reuters \n",
    "from nltk import FreqDist \n",
    "from nltk import word_tokenize, sent_tokenize \n",
    "from itertools import chain\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords') \n",
    "nltk.download('reuters') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK ngram functions\n",
    "\n",
    "There are several functions in NLTK that you can use. We start with `nltk.bigrams()`. It takes a list of tokens as input and gives you all the possible bigrams of words. Then we can compute the frequency distribution of those bigrams. \n",
    "\n",
    "But the best function is `everygrams`, which builds as many ngrams as you like from an input. You give it word tokens (but it can also be used with character tokens) and tell it how many types of ngrams to build. In my example, I say `1, 3`, which means: give me: unigrams, bigrams, trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"I really like you.\"\n",
    "\n",
    "sent1_tokens = nltk.word_tokenize(sent1)\n",
    "\n",
    "sent1_bi = nltk.bigrams(sent1_tokens)\n",
    "\n",
    "#compute frequency distribution for all the bigrams in the text\n",
    "sent1_fdist = nltk.FreqDist(sent1_bi)\n",
    "for key, value in sent1_fdist.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the same with your own sentence. \n",
    "# Copy the code from above to generate bigrams for sent2\n",
    "\n",
    "sent2 = \"I really really very much like you.\"\n",
    "\n",
    "sent2_tokens = nltk.word_tokenize(sent2)\n",
    "\n",
    "sent2_bi = nltk.bigrams(sent2_tokens)\n",
    "\n",
    "sent2_fdist = nltk.FreqDist(sent2_bi)\n",
    "for k, v in sent2_fdist.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, everygrams for sent1\n",
    "\n",
    "sent1_every = everygrams(sent1_tokens, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sent1_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build everygrams for your sent2\n",
    "# you can also build them of different length (1-2, 1-3, 1-4, etc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating ngram frequencies from Reuters\n",
    "\n",
    "Ngrams are really useful when we have large numbers of them and their frequencies. In this part, we take all the sentences in the Reuters corpus and count their frequencies. Then, we create a `removal_list` with all the things that we want to strip (punctuation and stopwords). Then, we create the lists of unigrams,  of bigrams and trigrams, padding to the left and to the right. Padding just means adding a special \"word\" that indicates the beginning and end of a sentence, so that the first and last words also participate in all possible bigrams. \n",
    "\n",
    "For instance, in _I really like you_, we could have the following bigrams:\n",
    "\n",
    "```\n",
    "I, really\n",
    "really, like\n",
    "like, you\n",
    "```\n",
    "\n",
    "But notice how, unlike the other words, _I_ and _you_ only participate in one bigram. We want to know that that's because they are the beginning and end of the sentence. Padding adds that information, which here I am representing with the html code `<s>` and `</s>`. So then we create the following bigrams:\n",
    "\n",
    "```\n",
    "<s>, I\n",
    "I, really\n",
    "really, like\n",
    "like, you\n",
    "you, </s>\n",
    "```\n",
    "\n",
    "So, we will first create a set of removal words, punctuation and stopwords that we don't want to include the in the lists of ngrams. You can see what it contains below. \n",
    "\n",
    "Next, we import the Reuters sentences and use `everygrams` to create unigrams, bigrams, and trigrams. We remove those that have words in the removal list. \n",
    "\n",
    "After that, `word_salad`is a dictionary with the frequency distribution of those ngrams. \n",
    "\n",
    "Finally, we use `word_salad` to create a sequence of segments that start with a certain prompt. The segments are made up of the prompt, plus the most likely next word. Thus, if the prompt is \"it will\", then we'll get the following:\n",
    "\n",
    "```\n",
    "('it', 'will'), \n",
    "('it', 'will', 'be'), \n",
    "('It', 'will'), \n",
    "('it', 'will', 'pay'), \n",
    "('it', 'will', 'not'), \n",
    "('IT', 'WILL'), \n",
    "('it', 'will', 'continue'), \n",
    "('it', 'will', 'have'), \n",
    "('it', 'will', 'make'), \n",
    "('it', 'will', 'take'), \n",
    "('It', 'will', 'be'), \n",
    "('it', 'will', 'raise'), \n",
    "('it', 'will', 'acquire'), \n",
    "('it', 'will', 'also'), \n",
    "('it', 'will', 'report'), \n",
    "('it', 'will', 'offer'), \n",
    "('it', 'will', 'issue'), \n",
    "('it', 'will', 'receive'), \n",
    "('it', 'will', 'increase'), \n",
    "('it', 'will', 'sell'),\n",
    " etc.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list of things we'll remove (punctuation and stopwords)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "string.punctuation = string.punctuation +'\"'+'\"'+'-'+'''+'''+'â€”'\n",
    "removal_list = list(stop_words) + list(string.punctuation)+ ['lt','rt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(removal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removal_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Reuters and create ngrams\n",
    "sents = reuters.sents()\n",
    "\n",
    "one_to_three_ngrams = chain(*[everygrams(sent, 1, 3, pad_left=True, pad_right=True) for sent in sents])\n",
    "one_to_three_ngrams = [ng for ng in one_to_three_ngrams if all(word for word in ng if word not in removal_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the frequency distribution, so that we can see which combos are more frequent\n",
    "word_salad = FreqDist(one_to_three_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_salad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dictionary in reverse order (the result is a list, but that's fine, as we only want to see it)\n",
    "word_salad_ordered = sorted(word_salad.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "# print the first 20 items\n",
    "word_salad_ordered[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an input \"prompt\"\n",
    "prefix = 'it will'\n",
    "\n",
    "# Check what's most possible to come next:\n",
    "print([ng for ng in word_salad if ' '.join(ng).lower().startswith(prefix.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a different prompt\n",
    "\n",
    "prefix2 = 'they said'\n",
    "\n",
    "# Check what's most possible to come next:\n",
    "print([ng for ng in word_salad if ' '.join(ng).lower().startswith(prefix2.lower())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Globe and Mail articles\n",
    "\n",
    "Here, we are going to do something a little different, based on a [notebook](https://www.kaggle.com/code/alvations/n-gram-language-model-with-nltk) on how to create a sentence generator from ngrams. \n",
    "\n",
    "Because this is a different section, I have all the separate import statements here, so that you see what we need. We will take a sample of article text from the Globe and Mail articles we worked on the other day (100 rows, but you can change that number).  We tokenize the text in the column `article_text`. \n",
    "\n",
    "Then, we preprocess the text and build what is essentially a language model (but not a _large_ language model) from the ngrams in the articles. A function produces sentences, one word at a time, from the frequencies in the ngrams. \n",
    "\n",
    "I hope you can see how this is a small step towards creating very large language models, simply based on the frequencies of sequences of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socc_df = pd.read_csv('data/gnm_articles.csv', encoding='utf-8', nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socc_comments =  list(socc_df['article_text'].apply(word_tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the tokenized text for 3-grams language modelling\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, socc_comments)\n",
    "train, vocab = padded_everygram_pipeline(2, socc_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a 3-grams model\n",
    "socc_model = MLE(n) \n",
    "socc_model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that generates sentences from a model\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed=42):\n",
    "    \"\"\"\n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param num_words: Max no. of words to generate.\n",
    "    :param random_seed: Seed value for random.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we use it to generate 'sentences' \n",
    "# Try and change the max number of words, or the random seed\n",
    "# Changing the random seed will give you a different sentence every time\n",
    "\n",
    "generate_sent(socc_model, num_words=100, random_seed=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
