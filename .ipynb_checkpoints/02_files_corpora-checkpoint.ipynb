{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading files, creating corpora\n",
    "\n",
    "Most of what we do in NLP is reading in files, counting things in them, and extracting information about their linguistic structure. In this notebook, we'll work on how to read a few files from the data/ directory and counting various things. \n",
    "\n",
    "So far, you have read in one file at a time, for instance the NYT file in the data directory, with the following command:\n",
    "\n",
    "`\n",
    "    with open(file='data/NYT_1991-01-16-A15.txt', mode='r', encoding='utf-8') as file:\n",
    "        nyt_text = file.read()\n",
    "`\n",
    "\n",
    "\n",
    "Now we want to read several files (or all the files) in a directory and work with them. We need the `Path` library, which we need to import first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a new variable, `data_dir`, and assign it the place where our data is. You can make this a relative path or an absolute path. Remember that, to know what your absolute path is, you can always execute the command `pwd` in a cell block. \n",
    "\n",
    "For more information on relative and absolute paths, review the reading from the POS module on [file organization](https://automatetheboringstuff.com/chapter8/).\n",
    "\n",
    "* Relative path: `Path('./data')`\n",
    "* Absolute path (this will change for your system): `Path('C:/Maite/MOD/notebooks/Ling450/data')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just print the names (and paths) of each of the files in that directory. This is just so that you know what's inside. The `glob()` function here matches all the files with a specific pattern. In this case, `'*'` means \"match anything\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filepath in Path(data_dir).glob('*'):\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have other files there, for instance, a file called `.ipynb_checkpoints`. You don't really want to read in that file, so you can restrict the listing to files that end in \".txt\" (i.e., that have the extension \"txt\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filepath in Path(data_dir).glob('*.txt'):\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading other files, using NLTK\n",
    "\n",
    "In this section, we are going to read in files from another directory and use NLTK to tokenize and count things in them. NLTK is a light-weight (but still quite powerful) NLP tool. For many purposes, you can use either NLTK or spaCy and you should be familiar with both. You can learn more about it from the [NLTK website](https://www.nltk.org/) and the [NLTK book](https://www.nltk.org/book/). \n",
    "\n",
    "If you have not, install NLTK by running the `nltk_install.ipynb` notebook in our [Ling450 GitHub repository](https://github.com/maitetaboada/Ling450). Once you have it installed NLTK in your system, you don't need to run that notebook again. You do need, however, to import it every time (with `import nltk`, as you see in the cell below). It's good practice to put all your import statements at the beginning of your notebook, so I'm going to put everything I'll need here. \n",
    "\n",
    "After you import nltk, you'll also need a few more of the NLTK libraries, so there are statements for that too. `numpy` and `matplotlib` are libraries for numerical calculation and for plotting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import matplotlib\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will point to a path where we have some files. For this exercise, I have placed some files in the `small_corpus/` directory in the Ling450 repository on GitHub. Make sure you download that entire directory to the same place where you have this notebook (the `./ ` part tells python to look in the current directory). The easiest way to get files from the class repository is to use [GitHub Desktop](https://desktop.github.com/). \n",
    "\n",
    "By the way, the texts in the `small_corpus` directory are from the [SFU Review Corpus](https://www.sfu.ca/~mtaboada/SFU_Review_Corpus.html). They are reviews of very old movies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_root = \"./small_corpus/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = PlaintextCorpusReader(corpus_root, '.*', encoding = \"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the variable 'reviews' is an object of type 'PlaintextCorpusReader'\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can list the files in that variable\n",
    "reviews.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to operate on the list of files. First, we will create a variable, `fileids`, to store the list of files in the reviews. \n",
    "\n",
    "Then, the dictionary `tokenized_reviews` will store the tokens for each file. Note that we create an empty dictionary first, to populate it inside the for loop.\n",
    "\n",
    "Finally, we go through each of the files in \"fileids\" and tokenize it, storing the information in the dictionary. In the next code block, you can see that the structure of `tokenized_reviews` is a dictionary with a structure where the key is the name of the file and the value a list of the tokens.\n",
    "\n",
    "`key: value`\n",
    "\n",
    "`bad_santa.txt: ['If', 'you', 'use', ...]`\n",
    "\n",
    "You can list the tokens alone by using the statement `tokenized_reviews[\"filename\"]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the names of the files int he corpus into a variable, 'fileids'\n",
    "fileids = reviews.fileids()\n",
    "\n",
    "# create an empty dictionary for the reviews once tokenized\n",
    "tokenized_reviews = {}\n",
    "\n",
    "# the for loop goes through each text (raw), tokenizes it (word_tokenize), and saves\n",
    "# the tokens for each file in the tokenized_reviews dictionary\n",
    "for fileid in fileids:\n",
    "    tokens = word_tokenize(reviews.raw(fileid))\n",
    "    tokenized_reviews[fileid] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_reviews[\"bad_santa.txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the tokens, we can calculate the types and lexical diversity (types / tokens). FreqDist is an NLTK function that gives you each type and the frequency it has it the text. You'll see that it's a dictionary, with the word/token as the key, and the count as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_reviews[\"bad_santa.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(tokenized_reviews[\"bad_santa.txt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(tokenized_reviews[\"bad_santa.txt\"])) / len(tokenized_reviews[\"bad_santa.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = FreqDist(tokenized_reviews[\"bad_santa.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn!\n",
    "\n",
    "Modify the for loop above, so you calculate all these things and print them for each file. You'll need to complete some of the code. Part of this is just a re-write of what's above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the names of the files in the corpus into a variable, 'fileids'\n",
    "fileids = reviews.fileids()\n",
    "\n",
    "# create an empty dictionary for the reviews once tokenized\n",
    "tokenized_reviews = {}\n",
    "\n",
    "# the for loop goes through each text (raw), tokenizes it (word_tokenize), and saves\n",
    "# the tokens for each file in the tokenized_reviews dictionary\n",
    "for fileid in fileids:\n",
    "    tokens = word_tokenize(reviews.raw(fileid))\n",
    "    tokenized_reviews[fileid] = tokens\n",
    "    token_count = len(tokenized_reviews[fileid])\n",
    "    # complete this line\n",
    "    type_count = \n",
    "    # complete this line\n",
    "    lex_div = \n",
    "    # complete this line\n",
    "    freq_dist = FreqDist(tokenized_reviews[fileid])\n",
    "    \n",
    "    print(\n",
    "        \"File: \", fileid, \"\\n\"\n",
    "        \"Token count: \", token_count, \"\\n\"\n",
    "        # complete the following few lines to print type and lexical diversity\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data from elsewhere\n",
    "\n",
    "There are lots of places to get data from. One easy place is [Project Gutenberg](https://www.gutenberg.org/), a free repository of files that are in the open domain. To access them, we will use another library, `requests`, which allows us to request information from places on the web.\n",
    "\n",
    "To get some sample files, I searched Project Gutenberg with the following parameters and selected five books from the results:\n",
    "\n",
    "* Search term: \"canada\"\n",
    "* Language: \"English\"\n",
    "* LoCC classification: \"P Language and Literatures: American and Canadian Literature\"\n",
    "* Filetype: Plain text UTF-8\n",
    "\n",
    "For other types of practice with Project Gutenberg data, you can also check out [Josef Fruehwald's data processing lesson](https://jofrhwld.github.io/teaching/courses/2022_lin517/lectures/data_processing/) and the [NLTK book, Chapter 3](https://www.nltk.org/book/ch03.html).\n",
    "\n",
    "We first put a few titles and their URLs (the plain text format link) into a dictionary, `canadiana`. Then, we go through the URL for each of the books in the dictionary and decode the text as UTF-8. The simple code below only gets the tokens and prints the length. If you also print the beginning of the text, you will see that it needs a lot of clean-up. For instance, all Project Gutenberg texts have a license at the beginning, which you probably don't want to count towards the token count of the book. There are a few other things that need processing too.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canadiana = {\"The Moccasin Maker\": \"https://www.gutenberg.org/cache/epub/6600/pg6600.txt\",\n",
    "             \"As Others See Us\": \"https://www.gutenberg.org/cache/epub/67312/pg67312.txt\",\n",
    "             \"An Algonquin Maiden\": \"https://www.gutenberg.org/cache/epub/8661/pg8661.txt\", \n",
    "             \"God's Green Country\": \"https://www.gutenberg.org/cache/epub/34700/pg34700.txt\", \n",
    "             \"The Blue Castle\": \"https://www.gutenberg.org/cache/epub/67979/pg67979.txt\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canadiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, url in canadiana.items():\n",
    "    response = urllib.request.urlopen(url)\n",
    "    raw = response.read().decode('utf8')\n",
    "    tokens = word_tokenize(raw)\n",
    "    \n",
    "    print(f\"Tokens for {key}: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
